Beginning AutoGluon training... Time limit = 1200s
AutoGluon will save models to 'c:\Users\Georgi\test_gluon\AutogluonModels\ag-20250207_142636'
=================== System Info ===================
AutoGluon Version:  1.2
Python Version:     3.9.10
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.19045
CPU Count:          8
GPU Count:          0
Memory Avail:       3.98 GB / 15.75 GB (25.3%)
Disk Space Avail:   85.65 GB / 237.84 GB (36.0%)
===================================================
Setting presets to: high_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': WQL,
 'freq': '15min',
 'hyperparameters': 'default',
 'known_covariates_names': ['temperature_2m',
                            'cloud_cover',
                            'cloud_cover_low',
                            'wind_speed_10m',
                            'direct_radiation',
                            'diffuse_radiation',
                            'global_tilted_irradiance'],
 'num_val_windows': 1,
 'prediction_length': 192,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'production',
 'time_limit': 1200,
 'verbosity': 2}

Provided train_data has 6432 rows, 1 time series. Median time series length is 6432 (min=6432, max=6432). 

Provided data contains following columns:
	target: 'production'
	known_covariates:
		categorical:        []
		continuous (float): ['temperature_2m', 'cloud_cover', 'cloud_cover_low', 'wind_speed_10m', 'direct_radiation', 'diffuse_radiation', ...]

To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit

AutoGluon will gauge predictive performance using evaluation metric: 'WQL'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2025-02-07 16:26:36
Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']
Training timeseries model SeasonalNaive. Training for up to 92.3s of the 1200.0s of remaining time.
	-0.9231       = Validation score (-WQL)
	0.03    s     = Training runtime
	4.89    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 99.6s of the 1195.0s of remaining time.
	-1.3365       = Validation score (-WQL)
	3.76    s     = Training runtime
	9.57    s     = Validation (prediction) runtime
Training timeseries model DirectTabular. Training for up to 107.4s of the 1181.6s of remaining time.
	-0.4135       = Validation score (-WQL)
	61.06   s     = Training runtime
	0.78    s     = Validation (prediction) runtime
Training timeseries model NPTS. Training for up to 112.0s of the 1119.8s of remaining time.
	-0.9104       = Validation score (-WQL)
	0.03    s     = Training runtime
	5.56    s     = Validation (prediction) runtime
Training timeseries model DynamicOptimizedTheta. Training for up to 123.8s of the 1114.2s of remaining time.
	-1.9577       = Validation score (-WQL)
	0.03    s     = Training runtime
	30.22   s     = Validation (prediction) runtime
Training timeseries model AutoETS. Training for up to 135.5s of the 1083.9s of remaining time.
	-1.3492       = Validation score (-WQL)
	0.03    s     = Training runtime
	6.06    s     = Validation (prediction) runtime
Training timeseries model ChronosZeroShot[bolt_base]. Training for up to 154.0s of the 1077.8s of remaining time.
	-0.5137       = Validation score (-WQL)
	0.04    s     = Training runtime
	2.86    s     = Validation (prediction) runtime
Training timeseries model ChronosFineTuned[bolt_small]. Training for up to 179.1s of the 1074.9s of remaining time.
	Fine-tuning on the CPU detected. We recommend using a GPU for faster fine-tuning of Chronos.
	Saving fine-tuned model to c:\Users\Georgi\test_gluon\AutogluonModels\ag-20250207_142636\models\ChronosFineTuned[bolt_small]\W0\fine-tuned-ckpt
	-0.5530       = Validation score (-WQL)
	153.44  s     = Training runtime
	0.54    s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 184.2s of the 920.9s of remaining time.
	-0.4916       = Validation score (-WQL)
	167.53  s     = Training runtime
	0.10    s     = Validation (prediction) runtime
Training timeseries model DeepAR. Training for up to 188.3s of the 753.2s of remaining time.
	-0.5676       = Validation score (-WQL)
	170.48  s     = Training runtime
	1.86    s     = Validation (prediction) runtime
Training timeseries model PatchTST. Training for up to 193.6s of the 580.8s of remaining time.
	-0.7843       = Validation score (-WQL)
	174.73  s     = Training runtime
	0.12    s     = Validation (prediction) runtime
Training timeseries model TiDE. Training for up to 202.9s of the 405.9s of remaining time.
	-0.6561       = Validation score (-WQL)
	184.65  s     = Training runtime
	0.21    s     = Validation (prediction) runtime
Fitting simple weighted ensemble.
	Ensemble weights: {'ChronosZeroShot[bolt_base]': 0.04, 'DeepAR': 0.09, 'DirectTabular': 0.75, 'PatchTST': 0.04, 'TemporalFusionTransformer': 0.07}
	-0.3961       = Validation score (-WQL)
	2.92    s     = Training runtime
	5.72    s     = Validation (prediction) runtime
Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE', 'WeightedEnsemble']
Total runtime: 982.08 s
Best model: WeightedEnsemble
Best model score: -0.3961
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble
