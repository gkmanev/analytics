{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and use deep learning model for PV production forecast\n",
    "### - Fetch historical PV production data from an API endpoint\n",
    "### - Fetch historical weather data and forecast from openmeto API:\n",
    "### - Make predictions for a future time interval\n",
    "### - Evaluate model accuracy\n",
    "-----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import or Install the needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pandas as pd_f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import openmeteo_requests\n",
    "\n",
    "import plotly.offline as pyo\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fetch the PV Production historical data from the API endpoint - http://209.38.208.230:8000/api/pvmeasurementdata/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "start_date = '2024-12-01'\n",
    "today = datetime.now().date() - timedelta(days=1)\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "# custom end date\n",
    "end_date = '2025-03-06'\n",
    "\n",
    "ppe='590310600032111448'\n",
    "\n",
    "url = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date}&end_date={end_date}&ppe={ppe}'\n",
    "\n",
    "# Get the data from the API\n",
    "response = requests.get(url=url)\n",
    "\n",
    "\n",
    "# Create a dataframe from the response\n",
    "\n",
    "df_dam = pd.DataFrame(response.json())\n",
    "\n",
    "\n",
    "# Clean the DataFrame\n",
    "df_dam['production'] = df_dam['production'].replace(['-', 'n/e', 'N/A', 'NaN'], np.nan)\n",
    "df_dam['production'] = df_dam['production'].astype(float)\n",
    "df_dam['timestamp'] = pd.to_datetime(df_dam['timestamp'], errors='coerce', utc=True)\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_localize(None)  \n",
    "\n",
    "\n",
    "# check for nans in the dataframe\n",
    "if df_dam.isnull().values.any():\n",
    "    print(\"Warning: NaNs detected in the DataFrame. Please fill or drop them.\")\n",
    "\n",
    "# drop all the columns instead of timestamp and production\n",
    "df_dam = df_dam[['timestamp', 'production', 'latitude', 'longitude']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               timestamp  production latitude longitude\n",
      "9020 2025-03-04 23:00:00         0.0  52.8953   18.3678\n",
      "9021 2025-03-04 23:15:00         0.0  52.8953   18.3678\n",
      "9022 2025-03-04 23:30:00         0.0  52.8953   18.3678\n",
      "9023 2025-03-04 23:45:00         0.0  52.8953   18.3678\n",
      "9024 2025-03-05 00:00:00         0.0  52.8953   18.3678\n"
     ]
    }
   ],
   "source": [
    "# just for checking\n",
    "print(df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make some visualisations of the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some initial data visualisations\n",
    "\n",
    "# start_week_number_analyzer = 1\n",
    "# end_week_number_analyzer = 52\n",
    "\n",
    "# df_dam_initial_chart = df_dam.copy()\n",
    "\n",
    "# df_dam_initial_chart['WeekNumber'] = df_dam_initial_chart['timestamp'].dt.isocalendar().week\n",
    "\n",
    "# # # Filter the data for the specified week range\n",
    "# df_weeks = df_dam_initial_chart[(df_dam_initial_chart['WeekNumber'] >= start_week_number_analyzer) & (df_dam_initial_chart['WeekNumber'] <= end_week_number_analyzer)]\n",
    "\n",
    "# # # Extract the date and hour for heatmap plotting\n",
    "# df_weeks['Date'] = df_weeks['timestamp'].dt.date\n",
    "# df_weeks['Hour'] = df_weeks['timestamp'].dt.hour\n",
    "\n",
    "# # Aggregate the prices by taking the average for each combination of Hour and Date\n",
    "# aggregated_data = df_weeks.groupby(['Hour', 'Date', 'WeekNumber'])['production'].mean().reset_index()\n",
    "\n",
    "# # Pivot the data to create a matrix suitable for heatmap\n",
    "# heatmap_data = aggregated_data.pivot(index='Hour', columns='Date', values='production')\n",
    "\n",
    "\n",
    "# # Create a heatmap\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#     z=heatmap_data.values,\n",
    "#     x=heatmap_data.columns,\n",
    "#     y=heatmap_data.index,\n",
    "#     colorscale='Viridis'\n",
    "# ))\n",
    "\n",
    "# # Update layout for better readability\n",
    "# fig.update_layout(\n",
    "#     title='DAM Market Price Heatmap',\n",
    "#     xaxis_title='Date',\n",
    "#     yaxis_title='Hour of Day',\n",
    "#     yaxis_nticks=24,\n",
    "#     xaxis_nticks=len(heatmap_data.columns),\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "# # Show the heatmap plot\n",
    "# fig.show()\n",
    "\n",
    "# # Create a line plot of hourly price over time\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(df_weeks['timestamp'], df_weeks['production'], linestyle=':', linewidth=1)\n",
    "# plt.xlabel('Date and Hour')\n",
    "# plt.ylabel('production')\n",
    "# plt.title('Hourly production Over Time')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fetch the historical weather data from open meto API - https://archive-api.open-meteo.com/v1/archive\n",
    "#### The weather data includes: \"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"\n",
    "#### We have also some data cleaning as well as resempling the data of 15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the weather data\n",
    "# # INCLUDE THE WEATHER FORECAST\n",
    "\n",
    "def fetch_weather_data(start, end, url_weather = \"https://archive-api.open-meteo.com/v1/archive\"):\n",
    "\n",
    "\tlat = float(df_dam['latitude'].iloc[0])\n",
    "\tlong = float(df_dam['longitude'].iloc[0])\n",
    "\t\n",
    "\t# start = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\tparams = {\n",
    "\t\t\"latitude\": lat,\n",
    "\t\t\"longitude\": long,\n",
    "\t\t\"start_date\":start,\t\n",
    "\t\t\"end_date\": end,\n",
    "\t\t\"hourly\": [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"],\n",
    "\t\t\"tilt\": 30\n",
    "\t}\n",
    "\t\n",
    "\tresponses = openmeteo.weather_api(url_weather, params=params)\n",
    "\tresponse_weather = responses[0]\n",
    "\t\n",
    "\n",
    "\n",
    "\thourly = response_weather.Hourly()\n",
    "\thourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\t\n",
    "\thourly_cloud_cover = hourly.Variables(1).ValuesAsNumpy()\n",
    "\thourly_cloud_cover_low = hourly.Variables(2).ValuesAsNumpy()\n",
    "\thourly_wind_speed_10m = hourly.Variables(3).ValuesAsNumpy()\n",
    "\thourly_direct_radiation = hourly.Variables(4).ValuesAsNumpy()\n",
    "\thourly_diffuse_radiation = hourly.Variables(5).ValuesAsNumpy()\n",
    "\thourly_global_tilted_irradiance = hourly.Variables(6).ValuesAsNumpy()\n",
    "\thourly_is_day = hourly.Variables(7).ValuesAsNumpy()\n",
    "\n",
    "\thourly_data = {\"date\": pd.date_range(\n",
    "\t\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\t\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\t\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\t\tinclusive = \"left\"\n",
    "\t)}\n",
    "\t\n",
    "\n",
    "\thourly_data[\"temperature_2m\"] = hourly_temperature_2m\t\n",
    "\thourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "\thourly_data[\"cloud_cover_low\"] = hourly_cloud_cover_low\n",
    "\thourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "\thourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
    "\thourly_data[\"diffuse_radiation\"] = hourly_diffuse_radiation\n",
    "\thourly_data[\"global_tilted_irradiance\"] = hourly_global_tilted_irradiance\n",
    "\thourly_data[\"is_day\"] = hourly_is_day\n",
    "\n",
    "\thourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "\t# Set index to datetime\n",
    "\thourly_dataframe[\"date\"] = pd.to_datetime(hourly_dataframe[\"date\"])\n",
    "\thourly_dataframe.set_index(\"date\", inplace=True)\n",
    "\n",
    "\t# Resample to 15-minute intervals using linear interpolation\n",
    "\tresampled_df = hourly_dataframe.resample(\"15T\").ffill()\n",
    "\n",
    "\t# Reset index to have 'date' as a column again\n",
    "\tresampled_df.reset_index(inplace=True)\n",
    "\n",
    "\tresampled_df[\"date\"] = resampled_df[\"date\"].dt.tz_localize(None)\n",
    "\t\n",
    "\t\n",
    "\n",
    "\treturn resampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Merge and preprocess the historical PV measurement data as well as the Weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 2025-03-05\n",
      "               timestamp  production  temperature_2m  cloud_cover  \\\n",
      "9019 2025-03-04 22:45:00         0.0          3.8415         27.0   \n",
      "9020 2025-03-04 23:00:00         0.0          3.3415         50.0   \n",
      "9021 2025-03-04 23:15:00         0.0          3.3415         50.0   \n",
      "9022 2025-03-04 23:30:00         0.0          3.3415         50.0   \n",
      "9023 2025-03-04 23:45:00         0.0          3.3415         50.0   \n",
      "\n",
      "      cloud_cover_low  wind_speed_10m  direct_radiation  diffuse_radiation  \\\n",
      "9019              0.0       14.650938               0.0                0.0   \n",
      "9020              0.0       15.503006               0.0                0.0   \n",
      "9021              0.0       15.503006               0.0                0.0   \n",
      "9022              0.0       15.503006               0.0                0.0   \n",
      "9023              0.0       15.503006               0.0                0.0   \n",
      "\n",
      "      global_tilted_irradiance  is_day  \n",
      "9019                       0.0     0.0  \n",
      "9020                       0.0     0.0  \n",
      "9021                       0.0     0.0  \n",
      "9022                       0.0     0.0  \n",
      "9023                       0.0     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_3908\\2320058220.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge the weather data with the production data\n",
    "\n",
    "print(start_date, end_date)\n",
    "\n",
    "resampled_df = fetch_weather_data(start_date, end_date)\n",
    "\n",
    "combined_weather_and_df_dam = pd.merge(df_dam, resampled_df, how='inner', left_on='timestamp', right_on='date')\n",
    "\n",
    "# Drop the duplicate date column\n",
    "combined_weather_and_df_dam.drop(columns='date', inplace=True)\n",
    "\n",
    "# Drop Latitude and Longitude columns\n",
    "combined_weather_and_df_dam.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Drop the rows with missing values\n",
    "combined_weather_and_df_dam.dropna(inplace=True)\n",
    "\n",
    "combined_weather_and_df_dam = combined_weather_and_df_dam.iloc[:-1]\n",
    "\n",
    "print(combined_weather_and_df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the future covariates for the length of the prediction interval - this includes the weather forecast with all weather parameters for the length of the future prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-05 2025-03-08\n",
      "288\n",
      "                              temperature_2m  cloud_cover  cloud_cover_low  \\\n",
      "item_id  timestamp                                                           \n",
      "series_1 2025-03-07 22:45:00          5.7305          0.0              0.0   \n",
      "         2025-03-07 23:00:00          5.4805          0.0              0.0   \n",
      "         2025-03-07 23:15:00          5.4805          0.0              0.0   \n",
      "         2025-03-07 23:30:00          5.4805          0.0              0.0   \n",
      "         2025-03-07 23:45:00          5.4805          0.0              0.0   \n",
      "\n",
      "                              wind_speed_10m  direct_radiation  \\\n",
      "item_id  timestamp                                               \n",
      "series_1 2025-03-07 22:45:00            3.60               0.0   \n",
      "         2025-03-07 23:00:00            3.96               0.0   \n",
      "         2025-03-07 23:15:00            3.96               0.0   \n",
      "         2025-03-07 23:30:00            3.96               0.0   \n",
      "         2025-03-07 23:45:00            3.96               0.0   \n",
      "\n",
      "                              diffuse_radiation  global_tilted_irradiance  \\\n",
      "item_id  timestamp                                                          \n",
      "series_1 2025-03-07 22:45:00                0.0                       0.0   \n",
      "         2025-03-07 23:00:00                0.0                       0.0   \n",
      "         2025-03-07 23:15:00                0.0                       0.0   \n",
      "         2025-03-07 23:30:00                0.0                       0.0   \n",
      "         2025-03-07 23:45:00                0.0                       0.0   \n",
      "\n",
      "                              is_day  \n",
      "item_id  timestamp                    \n",
      "series_1 2025-03-07 22:45:00     0.0  \n",
      "         2025-03-07 23:00:00     0.0  \n",
      "         2025-03-07 23:15:00     0.0  \n",
      "         2025-03-07 23:30:00     0.0  \n",
      "         2025-03-07 23:45:00     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_3908\\2320058220.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare future covariates\n",
    "# Fetch the /historical/ forecast data\n",
    "\n",
    "start_date_val = datetime.strptime(end_date, '%Y-%m-%d') \n",
    "start_date_val = start_date_val.strftime('%Y-%m-%d')\n",
    "end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=3)).strftime('%Y-%m-%d')\n",
    "print(start_date_val, end_date_val)\n",
    "\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d')\n",
    "\n",
    "# start_date_val = (datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "# end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# Rename Date to timestamp\n",
    "forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "forecast_df = forecast_df.iloc[:288]\n",
    "\n",
    "future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "    forecast_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "\n",
    "print(len(future_covariates))\n",
    "\n",
    "print(future_covariates.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the needed for the autugluon ML framework. Set the model - \"DeepAR\" as well as some hyperparameters and prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 600s\n",
      "AutoGluon will save models to 'c:\\Users\\Georgi\\test_gluon\\AutogluonModels\\ag-20250306_114656'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          8\n",
      "GPU Count:          0\n",
      "Memory Avail:       2.59 GB / 15.75 GB (16.4%)\n",
      "Disk Space Avail:   82.98 GB / 237.84 GB (34.9%)\n",
      "===================================================\n",
      "Setting presets to: fast_training\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': '15min',\n",
      " 'hyperparameters': 'very_light',\n",
      " 'known_covariates_names': ['temperature_2m',\n",
      "                            'cloud_cover',\n",
      "                            'cloud_cover_low',\n",
      "                            'wind_speed_10m',\n",
      "                            'direct_radiation',\n",
      "                            'diffuse_radiation',\n",
      "                            'global_tilted_irradiance',\n",
      "                            'is_day'],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 288,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'production',\n",
      " 'time_limit': 600,\n",
      " 'verbosity': 2}\n",
      "\n",
      "Provided train_data has 9024 rows, 1 time series. Median time series length is 9024 (min=9024, max=9024). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'production'\n",
      "\tknown_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['temperature_2m', 'cloud_cover', 'cloud_cover_low', 'wind_speed_10m', 'direct_radiation', 'diffuse_radiation', ...]\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-03-06 13:46:56\n",
      "Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta']\n",
      "Training timeseries model Naive. Training for up to 85.7s of the 600.0s of remaining time.\n",
      "\t-1.6336       = Validation score (-WQL)\n",
      "\t0.02    s     = Training runtime\n",
      "\t2.95    s     = Validation (prediction) runtime\n",
      "Training timeseries model SeasonalNaive. Training for up to 99.5s of the 597.0s of remaining time.\n",
      "\t-0.8185       = Validation score (-WQL)\n",
      "\t0.03    s     = Training runtime\n",
      "\t2.28    s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. Training for up to 118.9s of the 594.6s of remaining time.\n",
      "\t-1.0004       = Validation score (-WQL)\n",
      "\t2.42    s     = Training runtime\n",
      "\t12.56   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 144.9s of the 579.7s of remaining time.\n",
      "\t-0.5149       = Validation score (-WQL)\n",
      "\t28.11   s     = Training runtime\n",
      "\t0.55    s     = Validation (prediction) runtime\n",
      "Training timeseries model ETS. Training for up to 183.7s of the 551.0s of remaining time.\n",
      "\t-1.6341       = Validation score (-WQL)\n",
      "\t0.04    s     = Training runtime\n",
      "\t8.49    s     = Validation (prediction) runtime\n",
      "Training timeseries model Theta. Training for up to 271.2s of the 542.4s of remaining time.\n",
      "\t-3.7991       = Validation score (-WQL)\n",
      "\t0.05    s     = Training runtime\n",
      "\t43.53   s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 1.0}\n",
      "\t-0.5149       = Validation score (-WQL)\n",
      "\t1.28    s     = Training runtime\n",
      "\t0.55    s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'WeightedEnsemble']\n",
      "Total runtime: 102.58 s\n",
      "Best model: DirectTabular\n",
      "Best model score: -0.5149\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for the Autogluon\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "combined_weather_and_df_dam[\"item_id\"] = \"series_1\"\n",
    "\n",
    "target_column = 'production'  \n",
    "\n",
    "#Convert DataFrame to TimeSeriesDataFrame\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    combined_weather_and_df_dam,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "# model_path = \"AutogluonModels/ag-20250207_104540/\"  \n",
    "# predictor = TimeSeriesPredictor.load(model_path)\n",
    "\n",
    "\n",
    "#Initialize the predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    target=target_column,    \n",
    "    prediction_length=288,\n",
    "    freq='15min',\n",
    "    known_covariates_names=known_covariates,\n",
    "    #path=model_save_path  # Set the path here\n",
    ")\n",
    "\n",
    "#Fit the predictor with cross-validation\n",
    "results = predictor.fit(\n",
    "    train_data=train_data,    \n",
    "    time_limit=600,  # 20 min\n",
    "    presets='fast_training',\n",
    "    # eval_metric=\"MASE\",\n",
    "    #     hyperparameters={       \n",
    "    #     \"DeepAR\": {\n",
    "    #         \"num_evals\": 5,\n",
    "    #         \"epochs\": 100,\n",
    "    #         \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")]    \n",
    "                   \n",
    "    #                },\n",
    "            \n",
    "    # },\n",
    "    # hyperparameters={\n",
    "    #     \"DeepAR\": {\n",
    "    #         # You can specify DeepAR-specific hyperparameters here\n",
    "    #         # For example:\n",
    "    #         \"context_length\": 576,\n",
    "    #         \"num_layers\": 3,\n",
    "    #         \"hidden_size\": 96,\n",
    "    #         \"dropout_rate\": 0.1,\n",
    "    #         \"learning_rate\": 1e-3\n",
    "    #     }\n",
    "    # },  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the new covariates\n",
    "\n",
    "# start_date_val = '2025-02-07'\n",
    "# end_date_val = '2025-02-08'\n",
    "\n",
    "\n",
    "\n",
    "# forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# # Rename Date to timestamp\n",
    "# forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "# forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "# forecast_df = forecast_df.iloc[:96]\n",
    "\n",
    "# updated_train = combined_weather_and_df_dam.tail(96)\n",
    "\n",
    "\n",
    "# # offset the timestamp of updaited_train with + timedelta=(days=1)\n",
    "\n",
    "# updated_train['timestamp'] = updated_train['timestamp'] + timedelta(days=1)\n",
    "\n",
    "# forecast_df['timestamp'] = updated_train['timestamp'].values\n",
    "\n",
    "\n",
    "\n",
    "# future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "#     forecast_df,\n",
    "#     id_column=\"item_id\",\n",
    "#     timestamp_column=\"timestamp\"\n",
    "# )\n",
    "\n",
    "# known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "# print(len(future_covariates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: DirectTabular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n",
      "                              mean  0.1  0.2  0.3  0.4  0.5  0.6  \\\n",
      "item_id  timestamp                                                 \n",
      "series_1 2025-03-05 00:00:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-05 00:15:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-05 00:30:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-05 00:45:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-05 01:00:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...                            ...  ...  ...  ...  ...  ...  ...   \n",
      "         2025-03-07 22:45:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-07 23:00:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-07 23:15:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-07 23:30:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "         2025-03-07 23:45:00   0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "                                       0.7           0.8       0.9  \n",
      "item_id  timestamp                                                  \n",
      "series_1 2025-03-05 00:00:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-05 00:15:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-05 00:30:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-05 00:45:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-05 01:00:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "...                                    ...           ...       ...  \n",
      "         2025-03-07 22:45:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-07 23:00:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-07 23:15:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-07 23:30:00  3.594864e-09  2.268761e-08  0.017502  \n",
      "         2025-03-07 23:45:00  0.000000e+00  1.750229e-02  0.021217  \n",
      "\n",
      "[288 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# make the predictions\n",
    "print(len(future_covariates))\n",
    "predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "# predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "predictions[predictions < 0] = 0\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with known data of production for the predicted period:\n",
    "\n",
    "# start_date_val = end_date\n",
    "# end_date_val = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d') \n",
    "\n",
    "\n",
    "# url_val = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date_val}&end_date={end_date_val}&ppe={ppe}'\n",
    "# response_val = requests.get(url=url_val)\n",
    "# # Create a dataframe from the response\n",
    "# df_validation = pd.DataFrame(response_val.json())\n",
    "\n",
    "# df_validation['timestamp'] = pd.to_datetime(df_validation['timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_localize(None)  # Remove timezone\n",
    "\n",
    "# # prepare the prediction df\n",
    "# df_pred = predictions.reset_index()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot predicted mean\n",
    "# plt.plot(df_pred[\"timestamp\"], df_pred[\"mean\"], label=\"Predicted\", linestyle='--', color=\"red\", alpha=0.7)\n",
    "\n",
    "# plt.plot(df_validation[\"timestamp\"], df_validation[\"production\"], \n",
    "#              label=\"Actual\", linestyle='-', color=\"green\", alpha=0.7)\n",
    "\n",
    "# # Optionally plot prediction intervals\n",
    "# plt.xlabel(\"Timestamp\")\n",
    "# plt.ylabel(\"Production\")\n",
    "# plt.title(\"Actual vs. Predicted\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-07\n"
     ]
    }
   ],
   "source": [
    "# Generate ZUSE File\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Load your DataFrame (assuming you already have it)\n",
    "# forecast_data = pd.read_csv('your_data.csv')  # Uncomment if you need to load the data\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "workbook = load_workbook('ZUSE_template.xlsx')\n",
    "\n",
    "# Select the active sheet\n",
    "sheet = workbook.active\n",
    "\n",
    "df_pred = predictions.reset_index()\n",
    "forecast_data = df_pred.copy()\n",
    "\n",
    "# get last 96 values\n",
    "forecast_data = forecast_data['mean'].tail(96)\n",
    "\n",
    "\n",
    "# Get the data from the 'production_forecast' column\n",
    "data_to_write = forecast_data.tolist()\n",
    "\n",
    "date_to_zuse = datetime.strptime(end_date_val, '%Y-%m-%d') - timedelta(days=1)\n",
    "print(date_to_zuse.strftime('%Y-%m-%d'))\n",
    "\n",
    "# write end_date_val to the excel file on row 11, column 3\n",
    "sheet.cell(row=11, column=3, value=date_to_zuse)\n",
    "sheet.cell(row=11, column=1, value=ppe)\n",
    "# Write the data horizontally starting from row 11, column 4\n",
    "for col, value in enumerate(data_to_write, start=4):\n",
    "    sheet.cell(row=11, column=col, value=value)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save(f'ZUSE_{ppe}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  mean\n",
      "0   2025-03-05 00:00:00   0.0\n",
      "1   2025-03-05 00:15:00   0.0\n",
      "2   2025-03-05 00:30:00   0.0\n",
      "3   2025-03-05 00:45:00   0.0\n",
      "4   2025-03-05 01:00:00   0.0\n",
      "..                  ...   ...\n",
      "283 2025-03-07 22:45:00   0.0\n",
      "284 2025-03-07 23:00:00   0.0\n",
      "285 2025-03-07 23:15:00   0.0\n",
      "286 2025-03-07 23:30:00   0.0\n",
      "287 2025-03-07 23:45:00   0.0\n",
      "\n",
      "[288 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_pred[['timestamp','mean']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
