{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and use deep learning model for PV production forecast\n",
    "### - Fetch historical PV production data from an API endpoint\n",
    "### - Fetch historical weather data and forecast from openmeto API:\n",
    "### - Make predictions for a future time interval\n",
    "### - Evaluate model accuracy\n",
    "-----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import or Install the needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pandas as pd_f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import openmeteo_requests\n",
    "\n",
    "import plotly.offline as pyo\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fetch the PV Production historical data from the API endpoint - http://209.38.208.230:8000/api/pvmeasurementdata/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaNs detected in the DataFrame. Please fill or drop them.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "start_date = '2024-12-01'\n",
    "today = datetime.now().date() - timedelta(days=1)\n",
    "end_date = today.strftime('%Y-%m-%d')\n",
    "# custom end date\n",
    "end_date = '2025-03-10'\n",
    "\n",
    "ppe='590310600031289575'\n",
    "\n",
    "url = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date}&end_date={end_date}&ppe={ppe}'\n",
    "\n",
    "# Get the data from the API\n",
    "response = requests.get(url=url)\n",
    "\n",
    "\n",
    "# Create a dataframe from the response\n",
    "\n",
    "df_dam = pd.DataFrame(response.json())\n",
    "\n",
    "\n",
    "# Clean the DataFrame\n",
    "df_dam['production'] = df_dam['production'].replace(['-', 'n/e', 'N/A', 'NaN'], np.nan)\n",
    "df_dam['production'] = df_dam['production'].astype(float)\n",
    "df_dam['timestamp'] = pd.to_datetime(df_dam['timestamp'], errors='coerce', utc=True)\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_localize(None)  \n",
    "\n",
    "\n",
    "# check for nans in the dataframe\n",
    "if df_dam.isnull().values.any():\n",
    "    print(\"Warning: NaNs detected in the DataFrame. Please fill or drop them.\")\n",
    "\n",
    "# drop all the columns instead of timestamp and production\n",
    "df_dam = df_dam[['timestamp', 'production', 'latitude', 'longitude']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               timestamp  production latitude longitude\n",
      "9500 2025-03-09 23:00:00         0.0  52.8961   18.3745\n",
      "9501 2025-03-09 23:15:00         0.0  52.8961   18.3745\n",
      "9502 2025-03-09 23:30:00         0.0  52.8961   18.3745\n",
      "9503 2025-03-09 23:45:00         0.0  52.8961   18.3745\n",
      "9504 2025-03-10 00:00:00         NaN   0.0000    0.0000\n"
     ]
    }
   ],
   "source": [
    "# just for checking\n",
    "print(df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make some visualisations of the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some initial data visualisations\n",
    "\n",
    "# start_week_number_analyzer = 1\n",
    "# end_week_number_analyzer = 52\n",
    "\n",
    "# df_dam_initial_chart = df_dam.copy()\n",
    "\n",
    "# df_dam_initial_chart['WeekNumber'] = df_dam_initial_chart['timestamp'].dt.isocalendar().week\n",
    "\n",
    "# # # Filter the data for the specified week range\n",
    "# df_weeks = df_dam_initial_chart[(df_dam_initial_chart['WeekNumber'] >= start_week_number_analyzer) & (df_dam_initial_chart['WeekNumber'] <= end_week_number_analyzer)]\n",
    "\n",
    "# # # Extract the date and hour for heatmap plotting\n",
    "# df_weeks['Date'] = df_weeks['timestamp'].dt.date\n",
    "# df_weeks['Hour'] = df_weeks['timestamp'].dt.hour\n",
    "\n",
    "# # Aggregate the prices by taking the average for each combination of Hour and Date\n",
    "# aggregated_data = df_weeks.groupby(['Hour', 'Date', 'WeekNumber'])['production'].mean().reset_index()\n",
    "\n",
    "# # Pivot the data to create a matrix suitable for heatmap\n",
    "# heatmap_data = aggregated_data.pivot(index='Hour', columns='Date', values='production')\n",
    "\n",
    "\n",
    "# # Create a heatmap\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#     z=heatmap_data.values,\n",
    "#     x=heatmap_data.columns,\n",
    "#     y=heatmap_data.index,\n",
    "#     colorscale='Viridis'\n",
    "# ))\n",
    "\n",
    "# # Update layout for better readability\n",
    "# fig.update_layout(\n",
    "#     title='DAM Market Price Heatmap',\n",
    "#     xaxis_title='Date',\n",
    "#     yaxis_title='Hour of Day',\n",
    "#     yaxis_nticks=24,\n",
    "#     xaxis_nticks=len(heatmap_data.columns),\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "# # Show the heatmap plot\n",
    "# fig.show()\n",
    "\n",
    "# # Create a line plot of hourly price over time\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(df_weeks['timestamp'], df_weeks['production'], linestyle=':', linewidth=1)\n",
    "# plt.xlabel('Date and Hour')\n",
    "# plt.ylabel('production')\n",
    "# plt.title('Hourly production Over Time')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fetch the historical weather data from open meto API - https://archive-api.open-meteo.com/v1/archive\n",
    "#### The weather data includes: \"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"\n",
    "#### We have also some data cleaning as well as resempling the data of 15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the weather data\n",
    "# # INCLUDE THE WEATHER FORECAST\n",
    "\n",
    "def fetch_weather_data(start, end, url_weather = \"https://archive-api.open-meteo.com/v1/archive\"):\n",
    "\n",
    "\tlat = float(df_dam['latitude'].iloc[0])\n",
    "\tlong = float(df_dam['longitude'].iloc[0])\n",
    "\t\n",
    "\t# start = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\tparams = {\n",
    "\t\t\"latitude\": lat,\n",
    "\t\t\"longitude\": long,\n",
    "\t\t\"start_date\":start,\t\n",
    "\t\t\"end_date\": end,\n",
    "\t\t\"hourly\": [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"],\n",
    "\t\t\"tilt\": 30\n",
    "\t}\n",
    "\t\n",
    "\tresponses = openmeteo.weather_api(url_weather, params=params)\n",
    "\tresponse_weather = responses[0]\n",
    "\t\n",
    "\n",
    "\n",
    "\thourly = response_weather.Hourly()\n",
    "\thourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\t\n",
    "\thourly_cloud_cover = hourly.Variables(1).ValuesAsNumpy()\n",
    "\thourly_cloud_cover_low = hourly.Variables(2).ValuesAsNumpy()\n",
    "\thourly_wind_speed_10m = hourly.Variables(3).ValuesAsNumpy()\n",
    "\thourly_direct_radiation = hourly.Variables(4).ValuesAsNumpy()\n",
    "\thourly_diffuse_radiation = hourly.Variables(5).ValuesAsNumpy()\n",
    "\thourly_global_tilted_irradiance = hourly.Variables(6).ValuesAsNumpy()\n",
    "\thourly_is_day = hourly.Variables(7).ValuesAsNumpy()\n",
    "\n",
    "\thourly_data = {\"date\": pd.date_range(\n",
    "\t\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\t\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\t\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\t\tinclusive = \"left\"\n",
    "\t)}\n",
    "\t\n",
    "\n",
    "\thourly_data[\"temperature_2m\"] = hourly_temperature_2m\t\n",
    "\thourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "\thourly_data[\"cloud_cover_low\"] = hourly_cloud_cover_low\n",
    "\thourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "\thourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
    "\thourly_data[\"diffuse_radiation\"] = hourly_diffuse_radiation\n",
    "\thourly_data[\"global_tilted_irradiance\"] = hourly_global_tilted_irradiance\n",
    "\thourly_data[\"is_day\"] = hourly_is_day\n",
    "\n",
    "\thourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "\t# Set index to datetime\n",
    "\thourly_dataframe[\"date\"] = pd.to_datetime(hourly_dataframe[\"date\"])\n",
    "\thourly_dataframe.set_index(\"date\", inplace=True)\n",
    "\n",
    "\t# Resample to 15-minute intervals using linear interpolation\n",
    "\tresampled_df = hourly_dataframe.resample(\"15T\").ffill()\n",
    "\n",
    "\t# Reset index to have 'date' as a column again\n",
    "\tresampled_df.reset_index(inplace=True)\n",
    "\n",
    "\tresampled_df[\"date\"] = resampled_df[\"date\"].dt.tz_localize(None)\n",
    "\t\n",
    "\t\n",
    "\n",
    "\treturn resampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Merge and preprocess the historical PV measurement data as well as the Weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 2025-03-10\n",
      "               timestamp  production  temperature_2m  cloud_cover  \\\n",
      "9403 2025-03-08 22:45:00         0.0          5.4285          0.0   \n",
      "9404 2025-03-08 23:00:00         0.0          4.7285          0.0   \n",
      "9405 2025-03-08 23:15:00         0.0          4.7285          0.0   \n",
      "9406 2025-03-08 23:30:00         0.0          4.7285          0.0   \n",
      "9407 2025-03-08 23:45:00         0.0          4.7285          0.0   \n",
      "\n",
      "      cloud_cover_low  wind_speed_10m  direct_radiation  diffuse_radiation  \\\n",
      "9403              0.0       10.837435               0.0                0.0   \n",
      "9404              0.0        9.743305               0.0                0.0   \n",
      "9405              0.0        9.743305               0.0                0.0   \n",
      "9406              0.0        9.743305               0.0                0.0   \n",
      "9407              0.0        9.743305               0.0                0.0   \n",
      "\n",
      "      global_tilted_irradiance  is_day  \n",
      "9403                       0.0     0.0  \n",
      "9404                       0.0     0.0  \n",
      "9405                       0.0     0.0  \n",
      "9406                       0.0     0.0  \n",
      "9407                       0.0     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_16256\\2320058220.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge the weather data with the production data\n",
    "\n",
    "print(start_date, end_date)\n",
    "\n",
    "resampled_df = fetch_weather_data(start_date, end_date)\n",
    "\n",
    "\n",
    "combined_weather_and_df_dam = pd.merge(df_dam, resampled_df, how='inner', left_on='timestamp', right_on='date')\n",
    "\n",
    "# Drop the duplicate date column\n",
    "combined_weather_and_df_dam.drop(columns='date', inplace=True)\n",
    "\n",
    "# Drop Latitude and Longitude columns\n",
    "combined_weather_and_df_dam.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Drop the rows with missing values\n",
    "combined_weather_and_df_dam.dropna(inplace=True)\n",
    "\n",
    "combined_weather_and_df_dam = combined_weather_and_df_dam.iloc[:-4]\n",
    "\n",
    "print(combined_weather_and_df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the future covariates for the length of the prediction interval - this includes the weather forecast with all weather parameters for the length of the future prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 2025-03-12\n",
      "288\n",
      "                              temperature_2m  cloud_cover  cloud_cover_low  \\\n",
      "item_id  timestamp                                                           \n",
      "series_1 2025-03-09 00:00:00          3.4675          0.0              0.0   \n",
      "         2025-03-09 00:15:00          3.4675          0.0              0.0   \n",
      "         2025-03-09 00:30:00          3.4675          0.0              0.0   \n",
      "         2025-03-09 00:45:00          3.4675          0.0              0.0   \n",
      "         2025-03-09 01:00:00          2.9175          0.0              0.0   \n",
      "\n",
      "                              wind_speed_10m  direct_radiation  \\\n",
      "item_id  timestamp                                               \n",
      "series_1 2025-03-09 00:00:00        6.162207               0.0   \n",
      "         2025-03-09 00:15:00        6.162207               0.0   \n",
      "         2025-03-09 00:30:00        6.162207               0.0   \n",
      "         2025-03-09 00:45:00        6.162207               0.0   \n",
      "         2025-03-09 01:00:00        6.480000               0.0   \n",
      "\n",
      "                              diffuse_radiation  global_tilted_irradiance  \\\n",
      "item_id  timestamp                                                          \n",
      "series_1 2025-03-09 00:00:00                0.0                       0.0   \n",
      "         2025-03-09 00:15:00                0.0                       0.0   \n",
      "         2025-03-09 00:30:00                0.0                       0.0   \n",
      "         2025-03-09 00:45:00                0.0                       0.0   \n",
      "         2025-03-09 01:00:00                0.0                       0.0   \n",
      "\n",
      "                              is_day  \n",
      "item_id  timestamp                    \n",
      "series_1 2025-03-09 00:00:00     0.0  \n",
      "         2025-03-09 00:15:00     0.0  \n",
      "         2025-03-09 00:30:00     0.0  \n",
      "         2025-03-09 00:45:00     0.0  \n",
      "         2025-03-09 01:00:00     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_16256\\2320058220.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare future covariates\n",
    "# Fetch the /historical/ forecast data\n",
    "\n",
    "start_date_val = datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=1)\n",
    "start_date_val = start_date_val.strftime('%Y-%m-%d')\n",
    "end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=3)).strftime('%Y-%m-%d')\n",
    "print(start_date_val, end_date_val)\n",
    "\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d')\n",
    "\n",
    "# start_date_val = (datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "# end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# Rename Date to timestamp\n",
    "forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "forecast_df = forecast_df.iloc[:288]\n",
    "\n",
    "future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "    forecast_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "\n",
    "print(len(future_covariates))\n",
    "\n",
    "print(future_covariates.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the needed for the autugluon ML framework. Set the model - \"DeepAR\" as well as some hyperparameters and prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading predictor from path c:\\Users\\Georgi\\test_gluon\\AutogluonModels\\ag-20250310_110235\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for the Autogluon\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "combined_weather_and_df_dam[\"item_id\"] = \"series_1\"\n",
    "\n",
    "target_column = 'production'  \n",
    "\n",
    "#Convert DataFrame to TimeSeriesDataFrame\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    combined_weather_and_df_dam,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "model_path = \"AutogluonModels/ag-20250310_110235/\"  \n",
    "predictor = TimeSeriesPredictor.load(model_path)\n",
    "\n",
    "\n",
    "# #Initialize the predictor\n",
    "# predictor = TimeSeriesPredictor(\n",
    "#     target=target_column,    \n",
    "#     prediction_length=288,\n",
    "#     freq='15min',\n",
    "#     known_covariates_names=known_covariates,\n",
    "#     #path=model_save_path  # Set the path here\n",
    "# )\n",
    "\n",
    "# #Fit the predictor with cross-validation\n",
    "# results = predictor.fit(\n",
    "#     train_data=train_data,    \n",
    "#     time_limit=1200,  # 20 min\n",
    "#     #presets='fast_training',    \n",
    "#     hyperparameters={\n",
    "#                             \"DeepAR\": {\n",
    "#                                 # You can specify DeepAR-specific hyperparameters here\n",
    "#                                 # For example:\n",
    "#                                 \"context_length\": 396,\n",
    "#                                 \"num_layers\": 3,\n",
    "#                                 \"hidden_size\": 288,\n",
    "#                                 \"dropout_rate\": 0.2,\n",
    "#                                 \"learning_rate\": 1e-3,\n",
    "#                                 \"epochs\": 100,  # Added epochs parameter\n",
    "#                                 \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")]\n",
    "#                             }\n",
    "#     }                 \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the new covariates\n",
    "\n",
    "# start_date_val = '2025-02-07'\n",
    "# end_date_val = '2025-02-08'\n",
    "\n",
    "\n",
    "\n",
    "# forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# # Rename Date to timestamp\n",
    "# forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "# forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "# forecast_df = forecast_df.iloc[:96]\n",
    "\n",
    "# updated_train = combined_weather_and_df_dam.tail(96)\n",
    "\n",
    "\n",
    "# # offset the timestamp of updaited_train with + timedelta=(days=1)\n",
    "\n",
    "# updated_train['timestamp'] = updated_train['timestamp'] + timedelta(days=1)\n",
    "\n",
    "# forecast_df['timestamp'] = updated_train['timestamp'].values\n",
    "\n",
    "\n",
    "\n",
    "# future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "#     forecast_df,\n",
    "#     id_column=\"item_id\",\n",
    "#     timestamp_column=\"timestamp\"\n",
    "# )\n",
    "\n",
    "# known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "# print(len(future_covariates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: DeepAR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  mean  0.1  0.2  0.3  0.4       0.5  \\\n",
      "item_id  timestamp                                                     \n",
      "series_1 2025-03-09 00:00:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-09 00:15:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-09 00:30:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-09 00:45:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-09 01:00:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "...                                ...  ...  ...  ...  ...       ...   \n",
      "         2025-03-11 22:45:00  0.117117  0.0  0.0  0.0  0.0  0.028025   \n",
      "         2025-03-11 23:00:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-11 23:15:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-11 23:30:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-03-11 23:45:00  0.000000  0.0  0.0  0.0  0.0  0.000000   \n",
      "\n",
      "                                   0.6       0.7       0.8       0.9  \n",
      "item_id  timestamp                                                    \n",
      "series_1 2025-03-09 00:00:00  0.146898  0.296518  0.564850  1.165336  \n",
      "         2025-03-09 00:15:00  0.172285  0.346026  0.631387  1.050465  \n",
      "         2025-03-09 00:30:00  0.106849  0.271887  0.501149  0.919767  \n",
      "         2025-03-09 00:45:00  0.085514  0.329327  0.498948  0.927813  \n",
      "         2025-03-09 01:00:00  0.108369  0.302540  0.470958  0.770118  \n",
      "...                                ...       ...       ...       ...  \n",
      "         2025-03-11 22:45:00  0.150749  0.338858  0.748328  1.251333  \n",
      "         2025-03-11 23:00:00  0.155467  0.427483  0.692195  0.993617  \n",
      "         2025-03-11 23:15:00  0.000000  0.073143  0.261288  0.938009  \n",
      "         2025-03-11 23:30:00  0.089367  0.331088  0.622988  1.468847  \n",
      "         2025-03-11 23:45:00  0.163056  0.389703  0.762950  1.364927  \n",
      "\n",
      "[288 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# make the predictions\n",
    "print(len(future_covariates))\n",
    "predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "# predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "predictions[predictions < 0] = 0\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with known data of production for the predicted period:\n",
    "\n",
    "# start_date_val = end_date\n",
    "# end_date_val = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d') \n",
    "\n",
    "\n",
    "# url_val = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date_val}&end_date={end_date_val}&ppe={ppe}'\n",
    "# response_val = requests.get(url=url_val)\n",
    "# # Create a dataframe from the response\n",
    "# df_validation = pd.DataFrame(response_val.json())\n",
    "\n",
    "# df_validation['timestamp'] = pd.to_datetime(df_validation['timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_localize(None)  # Remove timezone\n",
    "\n",
    "# # prepare the prediction df\n",
    "# df_pred = predictions.reset_index()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot predicted mean\n",
    "# plt.plot(df_pred[\"timestamp\"], df_pred[\"mean\"], label=\"Predicted\", linestyle='--', color=\"red\", alpha=0.7)\n",
    "\n",
    "# plt.plot(df_validation[\"timestamp\"], df_validation[\"production\"], \n",
    "#              label=\"Actual\", linestyle='-', color=\"green\", alpha=0.7)\n",
    "\n",
    "# # Optionally plot prediction intervals\n",
    "# plt.xlabel(\"Timestamp\")\n",
    "# plt.ylabel(\"Production\")\n",
    "# plt.title(\"Actual vs. Predicted\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ZUSE File\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "# Load your DataFrame (assuming you already have it)\n",
    "# forecast_data = pd.read_csv('your_data.csv')  # Uncomment if you need to load the data\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "workbook = load_workbook('ZUSE_template.xlsx')\n",
    "\n",
    "# Select the active sheet\n",
    "sheet = workbook.active\n",
    "\n",
    "df_pred = predictions.reset_index()\n",
    "forecast_data = df_pred.copy()\n",
    "\n",
    "\n",
    "# get the values between 96 and 192\n",
    "forecast_data = forecast_data['mean'].iloc[96:192]\n",
    "\n",
    "#mean is always zero between timestamp 18:30 and 07:30\n",
    "forecast_data.loc[(forecast_data.index.time >= datetime.strptime('18:30', '%H:%M').time()) | \n",
    "                  (forecast_data.index.time <= datetime.strptime('07:30', '%H:%M').time()), 'mean'] = 0\n",
    "\n",
    "\n",
    "\n",
    "data_to_write = forecast_data.tolist()\n",
    "\n",
    "date_to_zuse = datetime.strptime(end_date_val, '%Y-%m-%d') - timedelta(days=1)\n",
    "\n",
    "# # write end_date_val to the excel file on row 11, column 3\n",
    "sheet.cell(row=11, column=3, value=date_to_zuse)\n",
    "sheet.cell(row=11, column=1, value=ppe)\n",
    "# Write the data horizontally starting from row 11, column 4\n",
    "for col, value in enumerate(data_to_write, start=4):\n",
    "    sheet.cell(row=11, column=col, value=value)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save(f'ZUSE_{ppe}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp      mean\n",
      "0   2025-03-09 00:00:00  0.000000\n",
      "1   2025-03-09 00:15:00  0.000000\n",
      "2   2025-03-09 00:30:00  0.000000\n",
      "3   2025-03-09 00:45:00  0.000000\n",
      "4   2025-03-09 01:00:00  0.000000\n",
      "..                  ...       ...\n",
      "283 2025-03-11 22:45:00  0.117117\n",
      "284 2025-03-11 23:00:00  0.000000\n",
      "285 2025-03-11 23:15:00  0.000000\n",
      "286 2025-03-11 23:30:00  0.000000\n",
      "287 2025-03-11 23:45:00  0.000000\n",
      "\n",
      "[288 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_pred[['timestamp','mean']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
