{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and use deep learning model for PV production forecast\n",
    "### - Fetch historical PV production data from an API endpoint\n",
    "### - Fetch historical weather data and forecast from openmeto API:\n",
    "### - Make predictions for a future time interval\n",
    "### - Evaluate model accuracy\n",
    "-----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import or Install the needed libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\test_gluon\\gluon_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pandas as pd_f\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "import openmeteo_requests\n",
    "\n",
    "import plotly.offline as pyo\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fetch the PV Production historical data from the API endpoint - http://209.38.208.230:8000/api/pvmeasurementdata/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: NaNs detected in the DataFrame. Please fill or drop them.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "start_date = '2024-12-01'\n",
    "today = datetime.now().date()\n",
    "end_date = today - timedelta(days=1)\n",
    "\n",
    "\n",
    "ppe='590310600032111448'\n",
    "\n",
    "url = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date}&end_date={end_date}&ppe={ppe}'\n",
    "\n",
    "# Get the data from the API\n",
    "response = requests.get(url=url)\n",
    "\n",
    "\n",
    "# Create a dataframe from the response\n",
    "\n",
    "df_dam = pd.DataFrame(response.json())\n",
    "\n",
    "\n",
    "# Clean the DataFrame\n",
    "df_dam['production'] = df_dam['production'].replace(['-', 'n/e', 'N/A', 'NaN'], np.nan)\n",
    "df_dam['production'] = df_dam['production'].astype(float)\n",
    "df_dam['timestamp'] = pd.to_datetime(df_dam['timestamp'], errors='coerce', utc=True)\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "df_dam['timestamp'] = df_dam['timestamp'].dt.tz_localize(None)  \n",
    "\n",
    "\n",
    "# check for nans in the dataframe\n",
    "if df_dam.isnull().values.any():\n",
    "    print(\"Warning: NaNs detected in the DataFrame. Please fill or drop them.\")\n",
    "\n",
    "# drop all the columns instead of timestamp and production\n",
    "df_dam = df_dam[['timestamp', 'production', 'latitude', 'longitude']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                timestamp  production latitude longitude\n",
      "15160 2025-05-07 23:00:00         0.0  52.8953   18.3678\n",
      "15161 2025-05-07 23:15:00         0.0  52.8953   18.3678\n",
      "15162 2025-05-07 23:30:00         0.0  52.8953   18.3678\n",
      "15163 2025-05-07 23:45:00         0.0  52.8953   18.3678\n",
      "15164 2025-05-08 00:00:00         NaN   0.0000    0.0000\n"
     ]
    }
   ],
   "source": [
    "# just for checking\n",
    "print(df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Make some visualisations of the historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Some initial data visualisations\n",
    "\n",
    "# start_week_number_analyzer = 1\n",
    "# end_week_number_analyzer = 52\n",
    "\n",
    "# df_dam_initial_chart = df_dam.copy()\n",
    "\n",
    "# df_dam_initial_chart['WeekNumber'] = df_dam_initial_chart['timestamp'].dt.isocalendar().week\n",
    "\n",
    "# # # Filter the data for the specified week range\n",
    "# df_weeks = df_dam_initial_chart[(df_dam_initial_chart['WeekNumber'] >= start_week_number_analyzer) & (df_dam_initial_chart['WeekNumber'] <= end_week_number_analyzer)]\n",
    "\n",
    "# # # Extract the date and hour for heatmap plotting\n",
    "# df_weeks['Date'] = df_weeks['timestamp'].dt.date\n",
    "# df_weeks['Hour'] = df_weeks['timestamp'].dt.hour\n",
    "\n",
    "# # Aggregate the prices by taking the average for each combination of Hour and Date\n",
    "# aggregated_data = df_weeks.groupby(['Hour', 'Date', 'WeekNumber'])['production'].mean().reset_index()\n",
    "\n",
    "# # Pivot the data to create a matrix suitable for heatmap\n",
    "# heatmap_data = aggregated_data.pivot(index='Hour', columns='Date', values='production')\n",
    "\n",
    "\n",
    "# # Create a heatmap\n",
    "# fig = go.Figure(data=go.Heatmap(\n",
    "#     z=heatmap_data.values,\n",
    "#     x=heatmap_data.columns,\n",
    "#     y=heatmap_data.index,\n",
    "#     colorscale='Viridis'\n",
    "# ))\n",
    "\n",
    "# # Update layout for better readability\n",
    "# fig.update_layout(\n",
    "#     title='DAM Market Price Heatmap',\n",
    "#     xaxis_title='Date',\n",
    "#     yaxis_title='Hour of Day',\n",
    "#     yaxis_nticks=24,\n",
    "#     xaxis_nticks=len(heatmap_data.columns),\n",
    "#     height=600\n",
    "# )\n",
    "\n",
    "# # Show the heatmap plot\n",
    "# fig.show()\n",
    "\n",
    "# # Create a line plot of hourly price over time\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(df_weeks['timestamp'], df_weeks['production'], linestyle=':', linewidth=1)\n",
    "# plt.xlabel('Date and Hour')\n",
    "# plt.ylabel('production')\n",
    "# plt.title('Hourly production Over Time')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Fetch the historical weather data from open meto API - https://archive-api.open-meteo.com/v1/archive\n",
    "#### The weather data includes: \"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"\n",
    "#### We have also some data cleaning as well as resempling the data of 15min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the weather data\n",
    "# # INCLUDE THE WEATHER FORECAST\n",
    "\n",
    "def fetch_weather_data(start, end, url_weather = \"https://archive-api.open-meteo.com/v1/archive\"):\n",
    "\n",
    "\tlat = float(df_dam['latitude'].iloc[0])\n",
    "\tlong = float(df_dam['longitude'].iloc[0])\n",
    "\t\n",
    "\t# start = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\tparams = {\n",
    "\t\t\"latitude\": lat,\n",
    "\t\t\"longitude\": long,\n",
    "\t\t\"start_date\":start,\t\n",
    "\t\t\"end_date\": end,\n",
    "\t\t\"hourly\": [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"],\n",
    "\t\t\"tilt\": 30\n",
    "\t}\n",
    "\t\n",
    "\tresponses = openmeteo.weather_api(url_weather, params=params)\n",
    "\tresponse_weather = responses[0]\n",
    "\t\n",
    "\n",
    "\n",
    "\thourly = response_weather.Hourly()\n",
    "\thourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\t\n",
    "\thourly_cloud_cover = hourly.Variables(1).ValuesAsNumpy()\n",
    "\thourly_cloud_cover_low = hourly.Variables(2).ValuesAsNumpy()\n",
    "\thourly_wind_speed_10m = hourly.Variables(3).ValuesAsNumpy()\n",
    "\thourly_direct_radiation = hourly.Variables(4).ValuesAsNumpy()\n",
    "\thourly_diffuse_radiation = hourly.Variables(5).ValuesAsNumpy()\n",
    "\thourly_global_tilted_irradiance = hourly.Variables(6).ValuesAsNumpy()\n",
    "\thourly_is_day = hourly.Variables(7).ValuesAsNumpy()\n",
    "\n",
    "\thourly_data = {\"date\": pd.date_range(\n",
    "\t\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\t\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\t\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\t\tinclusive = \"left\"\n",
    "\t)}\n",
    "\t\n",
    "\n",
    "\thourly_data[\"temperature_2m\"] = hourly_temperature_2m\t\n",
    "\thourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "\thourly_data[\"cloud_cover_low\"] = hourly_cloud_cover_low\n",
    "\thourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "\thourly_data[\"direct_radiation\"] = hourly_direct_radiation\n",
    "\thourly_data[\"diffuse_radiation\"] = hourly_diffuse_radiation\n",
    "\thourly_data[\"global_tilted_irradiance\"] = hourly_global_tilted_irradiance\n",
    "\thourly_data[\"is_day\"] = hourly_is_day\n",
    "\n",
    "\thourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "\t# Set index to datetime\n",
    "\thourly_dataframe[\"date\"] = pd.to_datetime(hourly_dataframe[\"date\"])\n",
    "\thourly_dataframe.set_index(\"date\", inplace=True)\n",
    "\t\n",
    "\t# Resample to 15-minute intervals using linear interpolation\n",
    "\tresampled_df = hourly_dataframe.resample(\"15T\").ffill()\n",
    "\n",
    "\t# Reset index to have 'date' as a column again\n",
    "\tresampled_df.reset_index(inplace=True)\n",
    "\n",
    "\tresampled_df[\"date\"] = resampled_df[\"date\"].dt.tz_localize(None)\t\n",
    "\t\n",
    "\t#print(resampled_df.tail())\n",
    "\n",
    "#fetch_weather_data(start_date, end_date - timedelta(days=1))\n",
    "\treturn resampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Merge and preprocess the historical PV measurement data as well as the Weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-01 2025-05-08\n",
      "                timestamp  production  temperature_2m  cloud_cover  \\\n",
      "15159 2025-05-07 22:45:00         0.0          3.9415         41.0   \n",
      "15160 2025-05-07 23:00:00         0.0          3.0915          7.0   \n",
      "15161 2025-05-07 23:15:00         0.0          3.0915          7.0   \n",
      "15162 2025-05-07 23:30:00         0.0          3.0915          7.0   \n",
      "15163 2025-05-07 23:45:00         0.0          3.0915          7.0   \n",
      "\n",
      "       cloud_cover_low  wind_speed_10m  direct_radiation  diffuse_radiation  \\\n",
      "15159              1.0        9.659814               0.0                0.0   \n",
      "15160              0.0        9.418661               0.0                0.0   \n",
      "15161              0.0        9.418661               0.0                0.0   \n",
      "15162              0.0        9.418661               0.0                0.0   \n",
      "15163              0.0        9.418661               0.0                0.0   \n",
      "\n",
      "       global_tilted_irradiance  is_day  \n",
      "15159                       0.0     0.0  \n",
      "15160                       0.0     0.0  \n",
      "15161                       0.0     0.0  \n",
      "15162                       0.0     0.0  \n",
      "15163                       0.0     0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_18352\\2068449019.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge the weather data with the production data\n",
    "\n",
    "print(start_date, end_date)\n",
    "\n",
    "resampled_df = fetch_weather_data(start_date, end_date)\n",
    "\n",
    "\n",
    "#print(df_dam.tail())\n",
    "\n",
    "combined_weather_and_df_dam = pd.merge(df_dam, resampled_df, how='inner', left_on='timestamp', right_on='date')\n",
    "\n",
    "# Drop the duplicate date column\n",
    "combined_weather_and_df_dam.drop(columns='date', inplace=True)\n",
    "\n",
    "# Drop Latitude and Longitude columns\n",
    "combined_weather_and_df_dam.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Drop the rows with missing values\n",
    "combined_weather_and_df_dam.dropna(inplace=True)\n",
    "\n",
    "combined_weather_and_df_dam = combined_weather_and_df_dam#.iloc[:-4]\n",
    "\n",
    "print(combined_weather_and_df_dam.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the future covariates for the length of the prediction interval - this includes the weather forecast with all weather parameters for the length of the future prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 2025-05-11\n",
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Georgi\\AppData\\Local\\Temp\\ipykernel_18352\\2068449019.py:58: FutureWarning:\n",
      "\n",
      "'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare future covariates\n",
    "# Fetch the /historical/ forecast data\n",
    "\n",
    "#start_date_val = datetime.strptime(end_date, '%Y-%m-%d') #+ timedelta(days=1)\n",
    "#start_date_val = start_date_val.strftime('%Y-%m-%d')\n",
    "#end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=3)).strftime('%Y-%m-%d')\n",
    "start_date_val = end_date\n",
    "end_date_val = end_date + timedelta(days=3)\n",
    "print(start_date_val, end_date_val)\n",
    "\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d')\n",
    "\n",
    "# start_date_val = (datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)).strftime('%Y-%m-%d')\n",
    "# end_date_val = (datetime.strptime(start_date_val, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# Rename Date to timestamp\n",
    "forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "forecast_df = forecast_df.iloc[:288]\n",
    "\n",
    "future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "    forecast_df,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "\n",
    "print(len(future_covariates))\n",
    "\n",
    "#print(future_covariates.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Prepare the needed for the autugluon ML framework. Set the model - \"DeepAR\" as well as some hyperparameters and prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training... Time limit = 600s\n",
      "AutoGluon will save models to 'c:\\Users\\Georgi\\test_gluon\\AutogluonModels\\ag-20250509_103045'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.9.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          8\n",
      "GPU Count:          0\n",
      "Memory Avail:       5.21 GB / 15.75 GB (33.1%)\n",
      "Disk Space Avail:   62.98 GB / 237.84 GB (26.5%)\n",
      "===================================================\n",
      "Setting presets to: fast_training\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'freq': '15min',\n",
      " 'hyperparameters': 'very_light',\n",
      " 'known_covariates_names': ['temperature_2m',\n",
      "                            'cloud_cover',\n",
      "                            'cloud_cover_low',\n",
      "                            'wind_speed_10m',\n",
      "                            'direct_radiation',\n",
      "                            'diffuse_radiation',\n",
      "                            'global_tilted_irradiance',\n",
      "                            'is_day'],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 288,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'production',\n",
      " 'time_limit': 600,\n",
      " 'verbosity': 2}\n",
      "\n",
      "train_data with frequency 'None' has been resampled to frequency '15min'.\n",
      "Provided train_data has 15168 rows (NaN fraction=0.1%), 1 time series. Median time series length is 15168 (min=15168, max=15168). \n",
      "\n",
      "Provided data contains following columns:\n",
      "\ttarget: 'production'\n",
      "\tknown_covariates:\n",
      "\t\tcategorical:        []\n",
      "\t\tcontinuous (float): ['temperature_2m', 'cloud_cover', 'cloud_cover_low', 'wind_speed_10m', 'direct_radiation', 'diffuse_radiation', ...]\n",
      "\n",
      "To learn how to fix incorrectly inferred types, please see documentation for TimeSeriesPredictor.fit\n",
      "\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'WQL'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "===================================================\n",
      "\n",
      "Starting training. Start time is 2025-05-09 13:30:46\n",
      "Models that will be trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta']\n",
      "Training timeseries model Naive. Training for up to 85.4s of the 598.1s of remaining time.\n",
      "\t-1.1545       = Validation score (-WQL)\n",
      "\t0.05    s     = Training runtime\n",
      "\t2.94    s     = Validation (prediction) runtime\n",
      "Training timeseries model SeasonalNaive. Training for up to 99.2s of the 595.1s of remaining time.\n",
      "\t-0.8943       = Validation score (-WQL)\n",
      "\t0.04    s     = Training runtime\n",
      "\t2.25    s     = Validation (prediction) runtime\n",
      "Training timeseries model RecursiveTabular. Training for up to 118.6s of the 592.8s of remaining time.\n",
      "\t-1.1394       = Validation score (-WQL)\n",
      "\t2.35    s     = Training runtime\n",
      "\t13.74   s     = Validation (prediction) runtime\n",
      "Training timeseries model DirectTabular. Training for up to 144.2s of the 576.7s of remaining time.\n",
      "\t-0.7042       = Validation score (-WQL)\n",
      "\t10.89   s     = Training runtime\n",
      "\t0.35    s     = Validation (prediction) runtime\n",
      "Training timeseries model ETS. Training for up to 188.5s of the 565.4s of remaining time.\n",
      "\t-1.1548       = Validation score (-WQL)\n",
      "\t0.06    s     = Training runtime\n",
      "\t6.02    s     = Validation (prediction) runtime\n",
      "Training timeseries model Theta. Training for up to 279.7s of the 559.3s of remaining time.\n",
      "\t-1.6112       = Validation score (-WQL)\n",
      "\t0.06    s     = Training runtime\n",
      "\t28.89   s     = Validation (prediction) runtime\n",
      "Fitting simple weighted ensemble.\n",
      "\tEnsemble weights: {'DirectTabular': 0.97, 'RecursiveTabular': 0.01, 'Theta': 0.02}\n",
      "\t-0.7022       = Validation score (-WQL)\n",
      "\t0.72    s     = Training runtime\n",
      "\t42.98   s     = Validation (prediction) runtime\n",
      "Training complete. Models trained: ['Naive', 'SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'WeightedEnsemble']\n",
      "Total runtime: 68.70 s\n",
      "Best model: WeightedEnsemble\n",
      "Best model score: -0.7022\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for the Autogluon\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "combined_weather_and_df_dam[\"item_id\"] = \"series_1\"\n",
    "\n",
    "target_column = 'production'  \n",
    "\n",
    "#Convert DataFrame to TimeSeriesDataFrame\n",
    "train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "    combined_weather_and_df_dam,\n",
    "    id_column=\"item_id\",\n",
    "    timestamp_column=\"timestamp\"\n",
    ")\n",
    "#future_covariates.head()\n",
    "#train_data.tail()\n",
    "\n",
    "# model_path = \"AutogluonModels/ag-20250311_093511/\"  \n",
    "# predictor = TimeSeriesPredictor.load(model_path)\n",
    "\n",
    "\n",
    "\n",
    "#Initialize the predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    target=target_column,    \n",
    "    prediction_length=288,\n",
    "    freq='15min',\n",
    "    known_covariates_names=known_covariates,\n",
    "    #path=model_save_path  # Set the path here\n",
    ")\n",
    "\n",
    "#Fit the predictor with cross-validation\n",
    "results = predictor.fit(\n",
    "    train_data=train_data,    \n",
    "    time_limit=600,  # 20 min\n",
    "    presets='fast_training',    \n",
    "    # hyperparameters={\n",
    "    #                         \"DeepAR\": {\n",
    "    #                             # You can specify DeepAR-specific hyperparameters here\n",
    "    #                             # For example:\n",
    "    #                             \"context_length\": 396,\n",
    "    #                             \"num_layers\": 3,\n",
    "    #                             \"hidden_size\": 288,\n",
    "    #                             \"dropout_rate\": 0.2,\n",
    "    #                             \"learning_rate\": 1e-3,\n",
    "    #                             \"epochs\": 100,  # Added epochs parameter\n",
    "    #                             \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")]\n",
    "    #                         }\n",
    "    # }                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the new covariates\n",
    "\n",
    "# start_date_val = '2025-02-07'\n",
    "# end_date_val = '2025-02-08'\n",
    "\n",
    "\n",
    "\n",
    "# forecast_df = fetch_weather_data(start_date_val, end_date_val, url_weather = \"https://api.open-meteo.com/v1/forecast\")\n",
    "\n",
    "# # Rename Date to timestamp\n",
    "# forecast_df.rename(columns={'date': 'timestamp'}, inplace=True)\n",
    "\n",
    "# forecast_df[\"item_id\"] = \"series_1\"\n",
    "\n",
    "# forecast_df = forecast_df.iloc[:96]\n",
    "\n",
    "# updated_train = combined_weather_and_df_dam.tail(96)\n",
    "\n",
    "\n",
    "# # offset the timestamp of updaited_train with + timedelta=(days=1)\n",
    "\n",
    "# updated_train['timestamp'] = updated_train['timestamp'] + timedelta(days=1)\n",
    "\n",
    "# forecast_df['timestamp'] = updated_train['timestamp'].values\n",
    "\n",
    "\n",
    "\n",
    "# future_covariates = TimeSeriesDataFrame.from_data_frame(\n",
    "#     forecast_df,\n",
    "#     id_column=\"item_id\",\n",
    "#     timestamp_column=\"timestamp\"\n",
    "# )\n",
    "\n",
    "# known_covariates = [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"wind_speed_10m\", \"direct_radiation\", \"diffuse_radiation\", \"global_tilted_irradiance\", \"is_day\"]\n",
    "\n",
    "# print(len(future_covariates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "data with frequency 'None' has been resampled to frequency '15min'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  mean  0.1  0.2  0.3  0.4       0.5  \\\n",
      "item_id  timestamp                                                     \n",
      "series_1 2025-05-08 00:00:00  0.002285  0.0  0.0  0.0  0.0  0.037101   \n",
      "         2025-05-08 00:15:00  0.001738  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-08 00:30:00  0.001234  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-08 00:45:00  0.000830  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-08 01:00:00  0.001178  0.0  0.0  0.0  0.0  0.000000   \n",
      "...                                ...  ...  ...  ...  ...       ...   \n",
      "         2025-05-10 22:45:00  0.012204  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-10 23:00:00  0.012091  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-10 23:15:00  0.012048  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-10 23:30:00  0.011804  0.0  0.0  0.0  0.0  0.000000   \n",
      "         2025-05-10 23:45:00  0.011314  0.0  0.0  0.0  0.0  0.000000   \n",
      "\n",
      "                                   0.6       0.7        0.8        0.9  \n",
      "item_id  timestamp                                                      \n",
      "series_1 2025-05-08 00:00:00  0.279298  0.531882   1.665399   2.877775  \n",
      "         2025-05-08 00:15:00  0.194199  0.573013   1.836700   3.161862  \n",
      "         2025-05-08 00:30:00  0.304611  0.693840   2.028091   3.491483  \n",
      "         2025-05-08 00:45:00  0.151446  0.667398   2.046300   3.384923  \n",
      "         2025-05-08 01:00:00  0.287939  0.820734   2.196350   3.908944  \n",
      "...                                ...       ...        ...        ...  \n",
      "         2025-05-10 22:45:00  2.632591  6.601241  10.993056  18.044953  \n",
      "         2025-05-10 23:00:00  2.548623  6.874119  11.033420  17.830509  \n",
      "         2025-05-10 23:15:00  2.742260  6.514126  10.873155  18.205194  \n",
      "         2025-05-10 23:30:00  2.841110  6.405385  11.326707  18.681439  \n",
      "         2025-05-10 23:45:00  2.984577  6.334432  11.068927  17.791160  \n",
      "\n",
      "[288 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# make the predictions\n",
    "print(len(future_covariates))\n",
    "predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "# predictions = predictor.predict(data=train_data, known_covariates=future_covariates)\n",
    "predictions[predictions < 0] = 0\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with known data of production for the predicted period:\n",
    "\n",
    "# start_date_val = end_date\n",
    "# end_date_val = datetime.strptime(end_date, '%Y-%m-%d') + timedelta(days=5)\n",
    "# end_date_val = end_date_val.strftime('%Y-%m-%d') \n",
    "\n",
    "\n",
    "# url_val = f'http://209.38.208.230:8000/api/pvmeasurementdata/?start_date={start_date_val}&end_date={end_date_val}&ppe={ppe}'\n",
    "# response_val = requests.get(url=url_val)\n",
    "# # Create a dataframe from the response\n",
    "# df_validation = pd.DataFrame(response_val.json())\n",
    "\n",
    "# df_validation['timestamp'] = pd.to_datetime(df_validation['timestamp'], errors='coerce', utc=True)\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_convert('Europe/Warsaw')\n",
    "\n",
    "# df_validation['timestamp'] = df_validation['timestamp'].dt.tz_localize(None)  # Remove timezone\n",
    "\n",
    "# # prepare the prediction df\n",
    "# df_pred = predictions.reset_index()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "\n",
    "# # Plot predicted mean\n",
    "# plt.plot(df_pred[\"timestamp\"], df_pred[\"mean\"], label=\"Predicted\", linestyle='--', color=\"red\", alpha=0.7)\n",
    "\n",
    "# plt.plot(df_validation[\"timestamp\"], df_validation[\"production\"], \n",
    "#              label=\"Actual\", linestyle='-', color=\"green\", alpha=0.7)\n",
    "\n",
    "# # Optionally plot prediction intervals\n",
    "# plt.xlabel(\"Timestamp\")\n",
    "# plt.ylabel(\"Production\")\n",
    "# plt.title(\"Actual vs. Predicted\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ZUSE File\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "# Load your DataFrame (assuming you already have it)\n",
    "# forecast_data = pd.read_csv('your_data.csv')  # Uncomment if you need to load the data\n",
    "\n",
    "# Load the existing Excel workbook\n",
    "workbook = load_workbook('ZUSE_template.xlsx')\n",
    "\n",
    "# Select the active sheet\n",
    "sheet = workbook.active\n",
    "\n",
    "df_pred = predictions.reset_index()\n",
    "forecast_data = df_pred.copy()\n",
    "\n",
    "\n",
    "# get the values between 96 and 192\n",
    "forecast_data = forecast_data['mean'].iloc[96:192]\n",
    "\n",
    "\n",
    "data_to_write = forecast_data.tolist()\n",
    "\n",
    "date_to_zuse = datetime.strptime(end_date_val, '%Y-%m-%d') - timedelta(days=1)\n",
    "\n",
    "# # write end_date_val to the excel file on row 11, column 3\n",
    "sheet.cell(row=11, column=3, value=date_to_zuse)\n",
    "sheet.cell(row=11, column=1, value=ppe)\n",
    "# Write the data horizontally starting from row 11, column 4\n",
    "for col, value in enumerate(data_to_write, start=4):\n",
    "    sheet.cell(row=11, column=col, value=value)\n",
    "\n",
    "# Save the workbook\n",
    "workbook.save(f'ZUSE_{ppe}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              timestamp  mean\n",
      "0   2025-03-10 00:00:00   0.0\n",
      "1   2025-03-10 00:15:00   0.0\n",
      "2   2025-03-10 00:30:00   0.0\n",
      "3   2025-03-10 00:45:00   0.0\n",
      "4   2025-03-10 01:00:00   0.0\n",
      "..                  ...   ...\n",
      "283 2025-03-12 22:45:00   0.0\n",
      "284 2025-03-12 23:00:00   0.0\n",
      "285 2025-03-12 23:15:00   0.0\n",
      "286 2025-03-12 23:30:00   0.0\n",
      "287 2025-03-12 23:45:00   0.0\n",
      "\n",
      "[288 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_pred[['timestamp','mean']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
